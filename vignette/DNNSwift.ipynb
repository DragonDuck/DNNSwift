{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DNNSwift\n",
    "\n",
    "<p align=\"center\" style=\"font-size:22;\">Rapid Prototyping of Deep Convolutional Neural Networks (DNN)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "This python package allows for the rapid training and application of deep neural networks (DNNs). It wraps the Tensorflow package, allowing for easy setup of an architecture without needing to know the nitty-gritty details of how Tensorflow operates.\n",
    "\n",
    "The focus of this package is to train and apply convolutional DNNs (CNN) with images. One-dimensional data could, hypothetically, be treated as a pseudo-image with the dimensions (N x 1 x 1) (spatial, spatial, channels). This should work but is currently untested.\n",
    "\n",
    "This vignette will present a example of how to use the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Package contents\n",
    "The primary class in the package is the 'DNNWrapper', which is the only class with which an end-user needs to directly interact.\n",
    "\n",
    "- DNNWrapper: A convenience class, which wraps around the interaction of the 'DNN' and 'ImageHandler' classes for the training and application use cases.\n",
    "- DNN: The core class of the package. This class acts as a container for the neural network structure and offers methods for training the network and applying it to individual images.\n",
    "- ImageHandler: This class handles the training data.\n",
    "- Validator: An auxillary class that calculates accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Installing the package\n",
    "The easiest way to install this package is via pip:\n",
    "\n",
    "    pip install dnnSwift\n",
    "\n",
    "Alternatively, you can download the 'dnnSwift' directory of this repository and place it in your python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Requirements\n",
    "The package was tested to work with Python 2.7.12 and requires several additional python packages to function properly:\n",
    "\n",
    "* tensorflow\n",
    "* time\n",
    "* os\n",
    "* sys\n",
    "* numpy\n",
    "* pickle\n",
    "* pygraphviz\n",
    "* h5py\n",
    "* hashlib\n",
    "\n",
    "All of these packages are either part of the python distribution or can be installed via pip:\n",
    "    \n",
    "    pip install <PACKAGE_NAME>\n",
    "\n",
    "Additionally, the package 'matplotlib' is required in order to run this vignette (but not for the actual package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quick start\n",
    "\n",
    "Training data for this package must be stored as a single [HDF5](https://support.hdfgroup.org/HDF5/) file. This repository contains a demo data set, a selection of the [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digit database for the digits \"0\", \"5\", and \"7\", which shows the expected structure. It can be found in the folder \"demo_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training a Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy: 0.948333333333\n",
      "Epoch 1: Accuracy: 0.983333333333\n",
      "Epoch 2: Accuracy: 0.985\n",
      "Epoch 3: Accuracy: 0.993333333333\n",
      "Epoch 4: Accuracy: 0.993333333333\n"
     ]
    }
   ],
   "source": [
    "import dnnSwift\n",
    "\n",
    "# The layout is a list of dictionaries defining each layer of the DNN. \n",
    "# For this example, the DNN will consist of two blocks of \n",
    "# \"convolution - convolution - maximum pooling\", followed by a fully \n",
    "# connected layer and finally a cross entropy cost function\n",
    "dnn_layout = [\n",
    "    {\n",
    "        \"name\": \"data\",\n",
    "        \"type\": \"input\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conv1\",\n",
    "        \"type\": \"conv\",\n",
    "        \"n_kernels\": 32,\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (1, 1),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"data\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conv2\",\n",
    "        \"type\": \"conv\",\n",
    "        \"n_kernels\": 32,\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (1, 1),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"conv1\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"maxpool1\",\n",
    "        \"type\": \"maxpool\",\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (2, 2),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"conv2\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conv3\",\n",
    "        \"type\": \"conv\",\n",
    "        \"n_kernels\": 64,\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (1, 1),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"maxpool1\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conv4\",\n",
    "        \"type\": \"conv\",\n",
    "        \"n_kernels\": 64,\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (1, 1),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"conv3\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"maxpool2\",\n",
    "        \"type\": \"maxpool\",\n",
    "        \"size\": (3, 3),\n",
    "        \"stride\": (2, 2),\n",
    "        \"padding\" : \"VALID\",\n",
    "        \"input\": \"conv4\"\n",
    "    },\n",
    "    {\"name\": \"fc1\", \"type\": \"fc\", \"input\": \"maxpool2\"},\n",
    "    {\"name\": \"CrossEntropy\", \"type\": \"cross_entropy\", \"input\": \"fc1\"}]\n",
    "\n",
    "# This is a dictionary that assigns numerical values to each label. The \n",
    "# keys of the dictionary are the actual labels used in the hdf5 file and \n",
    "# the values should be integery from 0 to N. Internally, labels are turned \n",
    "# into one-hot vectors, so in this set up, \"0\" becomes (1, 0, 0), \"5\" \n",
    "# becomes (0, 1, 0), and \"7\" becomes (0, 0, 1)\n",
    "categories = {\"0\": 0, \"5\": 1, \"7\": 2}\n",
    "\n",
    "# Initialize the DNN object.\n",
    "my_dnn = dnnSwift.DNNWrapper(\n",
    "    categories=categories, layout=dnn_layout)\n",
    "\n",
    "# The training data must be initialized. This means that the hdf5 file is \n",
    "# scanned and the images split into a training, validation, and test set.\n",
    "# The indices of the lists are written into the 'outfile' as a pickled \n",
    "# dictionary.\n",
    "my_dnn.initialize_training_data(\n",
    "    filename=\"demo_data/MNISTDemo.h5\", \n",
    "    outfile=\"demo_data/MNISTDemo_split.pkl\")\n",
    "\n",
    "# Train the network. Set 'verbose=True' to see the full output, including \n",
    "# the remaining time\n",
    "my_dnn.train_dnn(\n",
    "    num_epochs=5, batch_size=128, \n",
    "    weights_dir=\"demo_data/weights\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The _dnnSwift_ object writes its output into the 'weights_dir' directory. The output consists of weights and validation data. Validation data is calculated from data that was not trained on, i.e. it can be used for 1-fold cross validation of the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files: \n",
      "  - val_0.pkl\n",
      "  - val_1.pkl\n",
      "  - val_2.pkl\n",
      "  - val_3.pkl\n",
      "  - val_4.pkl\n",
      "  - weights_0.pkl\n",
      "  - weights_1.pkl\n",
      "  - weights_2.pkl\n",
      "  - weights_3.pkl\n",
      "  - weights_4.pkl\n",
      "  - weights_start.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# For each epoch, a weights file and a validation file are created. \n",
    "# Counting starts at 0, i.e. *_0.pkl is the output AFTER the first epoch!\n",
    "all_files = os.listdir(\"demo_data/weights\")\n",
    "print \"All Files: \"\n",
    "for f in all_files:\n",
    "    print \"  - %s\" % f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The validation files are pickled dictionaries containing various metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation file type: <type 'dict'>\n",
      "Validation file keys:\n",
      "  - recall\n",
      "  - precision\n",
      "  - rmse_loss\n",
      "  - top_n_acc\n",
      "  - top_1_acc\n",
      "  - counts\n",
      "  - data_counts\n",
      "  - cross_entropy_loss\n"
     ]
    }
   ],
   "source": [
    "with open(\"demo_data/weights/val_0.pkl\", \"r\") as f:\n",
    "    val_dat = pickle.load(f)\n",
    "    keys = val_dat.keys()\n",
    "    print \"Validation file type: \" + str(type(val_dat))\n",
    "    print \"Validation file keys:\"\n",
    "    for key in keys:\n",
    "        print \"  - %s\" % key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets determine the epoch with the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracies:\n",
      "  - 0.9483\n",
      "  - 0.9833\n",
      "  - 0.9850\n",
      "  - 0.9933\n",
      "  - 0.9933\n",
      "Best epoch: 3\n"
     ]
    }
   ],
   "source": [
    "val_accs = []\n",
    "for i in range(5):\n",
    "    with open(\"demo_data/weights/val_%s.pkl\" % str(i), \"r\") as f:\n",
    "        val_accs.append(pickle.load(f)[\"top_1_acc\"])\n",
    "best_epoch = val_accs.index(max(val_accs))\n",
    "print \"Validation accuracies:\"\n",
    "for val_acc in val_accs:\n",
    "    print \"  - %.4f\" % val_acc\n",
    "print \"Best epoch: %s\" % str(best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen that \"top_1_acc\" represents the top-1 accuracy. This is a common measure for determining the accuracy of neural networks. It represents the probability of the network correctly predicting the class of a validation data point. Additionally, the top-N accuracy is also calculated. This is the probability that the label of a validation data point is in the top N most likely classes predicted by the neural network.\n",
    "\n",
    "This is calculated for $1 <= N <= \\# Categories$. Here we have three categories into which we classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.948333333333\n",
      "Top-2 accuracy: 0.995\n",
      "Top-3 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with open(\"demo_data/weights/val_0.pkl\", \"r\") as f:\n",
    "    val = pickle.load(f)[\"top_n_acc\"]\n",
    "\n",
    "for key in val.keys():\n",
    "    print \"Top-%s accuracy: %s\" % (key, val[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Beyond that, we can also calculate the precision-recall curves from the \"precision\" and \"recall\" keys. These entries contain nested dictionaries and are calculated for each label independently. They are calculated for various probability thresholds. That means that instead of naively using the highest probability to determine the class, one can set custom thresholds for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1be56eed9f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Precision and Recall are calculated for each category individually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Labels: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_val' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Precision and Recall are calculated for each category individually\n",
    "print \"Labels: \" + str(best_val[\"precision\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to the precision and recall, the number of items predicted for each category at each threshold level is also stored. This can be useful for computing averages of precision and recall between categories. Note that this is **not** the total number of elements for each class in the validation data, but the number of elements predicted by the DNN, correct or not.\n",
    "\n",
    "Let's look at the precision-recall curves for the digit \"5\". Precision and Recall are calculated for various probability thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold | Precision |   Recall  | Counts\n",
      "--------------------------------------------\n",
      "  0.3333  |   0.8831  |   0.9808  |   231\n",
      "  0.3684  |   0.8831  |   0.9808  |   231\n",
      "  0.4035  |   0.8831  |   0.9808  |   231\n",
      "  0.4386  |   0.8831  |   0.9808  |   231\n",
      "  0.4737  |   0.8831  |   0.9808  |   231\n",
      "  0.5088  |   0.8904  |   0.9760  |   228\n",
      "  0.5439  |   0.8973  |   0.9663  |   224\n",
      "  0.5789  |   0.9000  |   0.9519  |   220\n",
      "  0.6140  |   0.9037  |   0.9471  |   218\n",
      "  0.6491  |   0.9206  |   0.9471  |   214\n",
      "  0.6842  |   0.9286  |   0.9375  |   210\n",
      "  0.7193  |   0.9327  |   0.9327  |   208\n",
      "  0.7544  |   0.9409  |   0.9183  |   203\n",
      "  0.7895  |   0.9397  |   0.8990  |   199\n",
      "  0.8246  |   0.9436  |   0.8846  |   195\n",
      "  0.8596  |   0.9468  |   0.8558  |   188\n",
      "  0.8947  |   0.9607  |   0.8221  |   178\n",
      "  0.9298  |   0.9620  |   0.7308  |   158\n",
      "  0.9649  |   0.9767  |   0.6058  |   129\n",
      "  1.0000  |      nan  |   0.0000  |     0\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We'll use the result of the first epoch to make the curve at least \n",
    "# somewhat interesting\n",
    "with open(\"demo_data/weights/val_0.pkl\", \"r\") as f:\n",
    "    val_dat = pickle.load(f)\n",
    "    precision = val_dat[\"precision\"][\"5\"]\n",
    "    recall = val_dat[\"recall\"][\"5\"]\n",
    "    counts = val_dat[\"counts\"][\"5\"]\n",
    "\n",
    "# The disadvantage of using a dictionary is that the entries are not \n",
    "# sorted. Future versions of this package may use a different storage \n",
    "# method for the validation\n",
    "p_thresh, p_val = zip(*precision.items())\n",
    "sort_indices = [p_thresh.index(p) for p in sorted(p_thresh, key=lambda x: float(x))]\n",
    "p_thresh = [float(p_thresh[i]) for i in sort_indices]\n",
    "p_val = [p_val[i] for i in sort_indices]\n",
    "    \n",
    "r_thresh, r_val = zip(*recall.items())\n",
    "sort_indices = [r_thresh.index(r) for r in sorted(r_thresh, key=lambda x: float(x))]\n",
    "r_thresh = [float(r_thresh[i]) for i in sort_indices]\n",
    "r_val = [r_val[i] for i in sort_indices]\n",
    "\n",
    "c_thresh, c_val = zip(*counts.items())\n",
    "sort_indices = [c_thresh.index(c) for c in sorted(c_thresh, key=lambda x: float(x))]\n",
    "c_thresh = [float(c_thresh[i]) for i in sort_indices]\n",
    "c_val = [c_val[i] for i in sort_indices]\n",
    "\n",
    "# The thresholds for all categories are identical:\n",
    "assert(p_thresh == r_thresh == c_thresh)\n",
    "\n",
    "# A value of 'nan' indicates that nothing was predicted, correctly or not, \n",
    "# with the given probability threshold.\n",
    "print \"Threshold | Precision |   Recall  | Counts\"\n",
    "print \"--------------------------------------------\"\n",
    "for i in range(len(p_thresh)):\n",
    "    print \"%8.4f  |\" % p_thresh[i], \\\n",
    "        \"%8.4f  |\" % p_val[i], \\\n",
    "        \"%8.4f  |\" % r_val[i], \\\n",
    "        \"%5d\" % c_val[i]\n",
    "print \"--------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports for this vignette\n",
    "These imports are used throughout the vignette. When running the code yourself, be sure to always run this block first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ImageHandler: Initializing and Handling of Training Data\n",
    "All training data is handled by the *ImageHandler* class. It expects training data to be in the [HDF5 file format](https://support.hdfgroup.org/HDF5/). The structure of this file must be as follows:\n",
    "\n",
    "    - FILE\n",
    "      - DATASET \"images\"\n",
    "        - SHAPE (Number of Images, Image Channels, Spatial Dimension, Spatial Dimension)\n",
    "      - DATASET \"labels\"\n",
    "        - SHAPE (Number of Images, 1)\n",
    "\n",
    "The dataset *images* effectively contains a 4D numpy array with the aforementioned shape. The dataset *labels* contains a 2D numpy array of label names, e.g.\n",
    "\n",
    "    array([['Label_A'],\n",
    "           ['Label_A'],\n",
    "           ['Label_B'],\n",
    "           ['Label_B'],\n",
    "           ['Label_B'],\n",
    "           ['Label_C'],\n",
    "           ['Label_C'],\n",
    "           ['Label_A']], \n",
    "          dtype='|S7')\n",
    "\n",
    "The *ImageHandler* class automatically turns the string labels into one-hot vectors, e.g. (1, 0, 0, 0), with the appropriate dimensions. Note that the dataset names, *images* and *labels* can be changed; custom names can be passed to the *ImageHandler* constructor.\n",
    "\n",
    "Note that the data file may, of course, have additional data sets (e.g. to store metadata). These are simply ignored.\n",
    "\n",
    "### Generating a Training Data HDF5 file\n",
    "For this demonstration, I generate a dataset consisting of noisy circles and squares, which we want to classify with a DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The size of our image segments (both dimensions have the same size)\n",
    "img_dims = 25\n",
    "\n",
    "# The number of images in each class\n",
    "num_images = 10000\n",
    "\n",
    "max_rad = ((img_dims - 1) // 2) - 1\n",
    "center = (img_dims - 1) // 2\n",
    "ind_y, ind_x = np.indices(dimensions=(img_dims, img_dims)) - center\n",
    "\n",
    "# Circles\n",
    "rand_rads = np.random.randint(5, max_rad+1, num_images)\n",
    "rand_intensities = np.random.randint(1, 256, num_images)\n",
    "noise_field = np.random.randint(-32, 33, (img_dims, img_dims, num_images))\n",
    "rad_field = np.repeat(np.expand_dims(np.sqrt(ind_y**2 + ind_x**2), 2), num_images, 2) <= rand_rads\n",
    "img_circles = (rad_field.astype(dtype=np.int16) * rand_intensities) + noise_field\n",
    "img_circles[img_circles < 0] = 0\n",
    "img_circles[img_circles > 255] = 255\n",
    "\n",
    "# Squares\n",
    "rand_rads = np.random.randint(5, max_rad+1, num_images)\n",
    "rand_intensities = np.random.randint(1, 256, num_images)\n",
    "noise_field = np.random.randint(-32, 33, (img_dims, img_dims, num_images))\n",
    "ind_x_square = np.repeat(np.expand_dims(ind_x, 2), num_images, 2)\n",
    "ind_y_square = np.repeat(np.expand_dims(ind_y, 2), num_images, 2)\n",
    "square_field = (np.abs(ind_x_square) <= rand_rads) * (np.abs(ind_y_square) <= rand_rads)\n",
    "img_squares = (square_field.astype(dtype=np.int16) * rand_intensities) + noise_field\n",
    "img_squares[img_squares < 0] = 0\n",
    "img_squares[img_squares > 255] = 255\n",
    "\n",
    "imgs = np.concatenate((img_circles, img_squares), 2)\n",
    "imgs = np.transpose(imgs, (2, 0, 1))\n",
    "imgs = np.expand_dims(imgs, 1)\n",
    "imgs = imgs.astype(np.uint8)\n",
    "labels = np.expand_dims(np.repeat((\"Circle\", \"Square\"), (num_images, num_images)), 1)\n",
    "\n",
    "with h5py.File(\"TrainingData.h5\", \"w\") as h5handle:\n",
    "    h5handle.create_dataset(name=\"images\", data=imgs, chunks=(1, 1, img_dims, img_dims), compression=3)\n",
    "    h5handle.create_dataset(name=\"labels\", data=labels, chunks=(1, 1), compression=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The resulting file, *TrainingData.h5*, has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  [u'images', u'labels']\n",
      "<HDF5 dataset \"images\": shape (20000, 1, 25, 25), type \"|u1\">\n",
      "<HDF5 dataset \"labels\": shape (20000, 1), type \"|S6\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"TrainingData.h5\", \"r\") as h5handle:\n",
    "    print \"Entries: \", h5handle.keys()\n",
    "    print h5handle[\"images\"]\n",
    "    print h5handle[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading the Training Data\n",
    "\n",
    "*ImageHandler* reads this file and prepares the data for the DNN class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import DNNSwift\n",
    "img_handler = DNNSwift.ImageHandler.initialize(\n",
    "    filename=\"TrainingData.h5\", \n",
    "    categories={\"Circle\": 0, \"Square\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The *categories* argument assigns numerical values to each label. The output of a tensorflow neural network is a vector of numbers, which can be interpreted as probabilities after the proper normalization. The integer value of the *categories* dictionary indicates the order of labels, i.e. in this case 'Circle' is assigned to the one-hot vector (1,0) and 'Square' to the one-hot vector (0,1). A normalized output of (0.4,0.6) would then be interpreted as a 60% probability for the input image being a square.\n",
    "\n",
    "*ImageHandler* randomly splits apart the entire dataset into three groups for training, (cross-)validation, and testing and internally stores the image indices belonging to each group. Note that the order of images is randomized, during this split so that the data order in the hdf5 file is not preserved at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :: Length: 2000\n",
      "train :: Length: 16000\n",
      "val :: Length: 2000\n"
     ]
    }
   ],
   "source": [
    "for list_name in img_handler.get_list_names():\n",
    "    print list_name, \":: Length:\", img_handler.get_list_length(list_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "By default, the split is 80%-10%-10% (Training, Validation, Testing), but this can be customized via the *data_split* argument of the constructor. It expects a list of values indicating the relative sizes of each image set (in order: training, validation, testing). The numbers are normalized internally so can be arbitrarily large. \n",
    "\n",
    "Likewise, the custom names for the HDF5 datasets can also be passed to the *ImageHandler* constructor via the arguments *image_key* and *label_key*, if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_handler = DNNSwift.ImageHandler.initialize(\n",
    "    filename=\"TrainingData.h5\", \n",
    "    categories={\"Circle\": 0, \"Square\": 1},\n",
    "    data_split=[60, 20, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The image split can be saved. If, for whatever reason, the network needs to be revalidated or training is interrupted and continued, the identical training dataset must be used. For this reason, *ImageHandler* can also be initialized with an index list, preventing the creation of a new one.\n",
    "\n",
    "In order to ensure that the HDF5 file with the training data is unchanged, the md5sum of this file is calculated and stored in the file containing the image group split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_handler.save_lists(\"IndexLists.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['h5file_hash', 'index_list']\n",
      "Index Lists:\n",
      "   test : [10555 13641  8198 ...,  9972 10937 13895]\n",
      "   train : [ 5994 15295  4350 ...,  5423  1479  3371]\n",
      "   val : [15712    82  3498 ...,  7921  1774  7309]\n",
      "md5sum: 885cd4ffb20867e700a43905179c284a\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"IndexLists.pkl\", \"r\") as f:\n",
    "    index_file = pickle.load(f)\n",
    "    \n",
    "print \"Keys:\", index_file.keys()\n",
    "print \"Index Lists:\"\n",
    "for key in index_file[\"index_list\"].keys():\n",
    "    print \"  \", key, \":\", index_file[\"index_list\"][key]\n",
    "print \"md5sum:\", index_file[\"h5file_hash\"]\n",
    "\n",
    "img_handler = DNNSwift.ImageHandler.initialize_with_index_list(\n",
    "    filename=\"TrainingData.h5\", \n",
    "    categories={\"Circle\": 0, \"Square\": 1}, \n",
    "    index_list=index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Images can be extracted directly from the image handler. This returns a custom object with the attributes \"imgs\" and \"labels\", which hold the image and label data, respectively. Note that *index_high* is an inclusive index, as opposed to python's usual indexing logic of excluding the upper limit. Ergo, setting \n",
    "\n",
    "    index_low == index_high \n",
    "\n",
    "returns a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: <DNNSwift.ImageHandler.ImagesAndLabels object at 0x13467d890>\n",
      "images.imgs: <type 'numpy.ndarray'> ; (10, 25, 25, 1)\n",
      "images.labels: <type 'numpy.ndarray'> ; (10, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x134d07750>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABECAYAAACRbs5KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXmQHcl93/nJqnr30efrbvTdDXRjAJBzcIYzwyF1mJQl\nUiuL0mrDouRYrR3ycjcsaa2wV7dsiRuUbVm2JUtLWUFLNE2bu6JC1kpcrcIUSXtESpwZzow4xAyA\nAdBAo4G+r9fv7HdUVe4fmXW9boAYHhgZU98IRKPee1WVlZX5y9/v+ztSSCmJESNGjBj/7cN4vRsQ\nI0aMGDG+PogFeowYMWLcJ4gFeowYMWLcJ4gFeowYMWLcJ4gFeowYMWLcJ4gFeowYMWLcJ/iaBLoQ\n4t1CiMtCiCUhxE9/vRoVI0aMGDFeO8RXG4cuhDCBK8BfB1aB54EfkFJe/Po1L0aMGDFi3C2+Fg39\ncWBJSnldStkBfhd479enWTFixIgR47XC+hrOnQBuhY5XgSfudELSysq0k/aPhWUiHVcd9FgKIp0C\nKZHtTvBb20EYag2Srnv0BgLoMThEOqV+32ojEglktxt8l0zq/+mTvGu32nd6DEQygex0/bZgCKTt\nRK4rO50j7YhcVwj/mUUqBd2u/0wiYSG7NsJSr0fatv48oY5Dz3DcsyIlSPw2CNMEywz6Ul//2GuY\nBtJxj947pa9td8E0g/N735tlgmmCo/vDlUjXDfpKP/ex7y98ndCzHum73u/12Ai+BJHU773dDvoA\nIGFFrqWeS0b7ylC/lZ0OIpnwfys7XX1OUl87+o6PPEMyAbYTvNfjnqNnrAjDgITlX18I4R97bbjT\n/YVlgWkE7Q29H5FKqmv2vFuE0BeXamzovhSG8OcEphntN2/uemPYUv0mux3/uXBl0JZWG2FZwXjq\nfWd3AZFOgT4nfB0ALOto+2xHyQQ4Khd0X/jHyQQI8RXf6bHtMk2kc/fPouSODMaTENyJKRGWSdXe\n3ZVSlr7Stb8WgX5XEEK8H3g/QJosT4h3BV+G+sCan8FeXvEFh3FyBufileCFOGCdGMXe3PJ/T6uN\nvbEJgHlmAYRQ54RgTs+r06+tgO1gnltU7arUsTfUtXAdjEIBt1bTjdbnDg+pc3f3sOZncW6uqi8M\nE2l0MLJZdXqjgTU7hb2i1jchLaSwsaYmAbBvrUI7dN3Tp3AuL/nHwrUwFmZwrlxTzzY2AckE7vqm\nbl5L/VDPP/NNp2FtE+egoo4XT4LrIupN9TPdR37fuWAWh3Ar6vmMTBqn3sA8NaueT98XwJyfw1la\nRlhaIDp6kuhxbs2p9+RDqM88oSATFjgOztJy5DfmyTl1r6VlzGIx+KqQx15bJwxrYjz4TABtME+f\nVI+yfEsJDW89EmAWB2B8RF3/wmV1jfFp9fu9Mm6thlnsU98fVLAmg+sLNxAy6rxJZD6jzl1aQXY7\nGGmlhLiiBUJgLixG7mWeO+0fWzPBODAzRZxqFXNEzUNnZyd4J4CRy+E2GpHPzLOnwZvoXRv7xk3M\njOovp1rF7CvCiBqXwnGxl1ew5mdV31eqOHv7mA8E7THy+h6AeXIB59JV/3s6XUSrg8zod31zDWOg\nH7dSVe0bG8G+fkP91ibSTn/uemN6YAiRz/kC115dC86DoM+938/P41y9rsYu0TGobm6CGwgI68SY\nP9e9+0Y+c9Rn4Tm79KtPcuofPKvO1/PTOjGmmrWxibk4D/sH6vd7+5HnEZaFOTWBu7vv39KXD4A5\nNOifY/YN4JTL3A7WxDiymEOuqD5xm2qeWlMTfl+F5Yd1YgzSKZzVDdUVp2b41MV/unLMpY/ga6Fc\n1oCp0PGk/iwCKeWHpZSPSSkfS4jU13C7GDFixIhxJ3wtTlEL5RR9F0qQPw/8oJTywu3O6UuNyse7\n3xz5zNd+Wi2l0Wrz0rm17ptvANbYKM7uHsb8jPpg/wBndw/zlNL8OKji7O5Frm0ODIA2mWWlisjn\njvwGwJqdxr5xM2hTLocx0I+jNV1p25hnlHYDagWV/QX/2DyzADv7iIzS7Oxbq5jFom9Ku60WIpHE\nOKXb3rURjUNkIaf6stON3N9vfymk2fUipMEY2ay/6gOIRBLZ7QR9k0oib65HNIw7wZqcQB4eqmtl\nMtira6ov4Y6aSKR5hQIQaDXh882zi8jr6nndVuuO1zEXtCantXpp25FnBfW+PIrHu541o3QNe+UW\n5sI8ck1bO82men+DSmOXN1Z9DfbY+4feey+syQlkOhmxRsxTc/6xsCxlius5ZhaVxh6mOLzPQWvg\n/X2+5WXNzSCTCd/yIplAVmuBNgkRy9IcGEDkc8i0omScq9fVdTwNvtZApFPKYvTaZ9vKwgKc1Q2M\nuamoxXYqsKx6tebetluTE6CtHc9K9MaByGZwtraP0j3edUZHELmsbxGYoyNQyPl96Wmt7paaC94Y\n8DTyS780z9ALFt2s6tvqGZvEgUlmUx27CUhWJM0xdTz/4WvYm1sBIzAzedRK6IEnq0CNs/D8tOZm\n/H6Xq5sIIdS79s7N5fxn9mhA38LvGc/eGPD7ypV8xvnEi1LKx+7YQL4GykVKaQshfhT4FGACH7mT\nMAfN6emxbE1NYt9a9SegOTqiJocehEeQzSBt+2inH6hO6xXUvdSANTaKW40KNHNUmen2jZsRoSgm\nxrDDg7q/LzKp3UoVN2T+OZeuYs1Og8dRp1IwdQJjX01MKlXcZhNRVm2VUmJv72D2n/Lvb01NIgvq\nBbO9h1urQ0kJQXoEutnfByPDiIYSuh59YJ5W1xOdLvbySpT2QJmJEJiX3mQQ6XRgIoN6D/rYE4xh\nQe5TBd73QvgLklEoINLpo4uQHpzmudOwvY/I6WftEegikcSYmUAcaqqna2MunvTfu1kqQa9AH+j3\nF26RSuBcuopsasE+MY7cLUcmjb2xCR4t1aPQmMNDkbEUfu/m8BB0usFETVi4NwI3kpHNKkqpZ/Ez\nzyyotjVbmAnLpzQwTWS7HZn4JJL+f73xa02MR449moKdfbVAekK1XIZy2acVrMkJZDGHrWlIs78P\ne2cnIqTNs4vB90ODPgUBwWIK0Xfuo0dohceQOaBoCH+xuY0g9/0VtTrO1rb/sbO9A1vbvhC1NzYj\nfieAnf/1bVQeUMf5awatIegW1HHfBYv6tMRsqWM3ITgcEQh9+vLfPYnZOsn4v/iCepTmUcXCLJWC\ncWyYapx5bQEY7tcN2cG5tXbk2cKLlxgfxdV9aY6O4GxtHxHk/nn9fXBQOdpXd4GvWkP/alAUg/Kp\nN/09IMQ/9ggZv2GplL+SQbBqmWc1f/nqtYi24HW+t2qKXAZpmUeE2t0irLVbE+PgulEOLwTz7CJs\n7QaC8hitLjwhzHOn/eeHQKP2JiLJBM76lm+hWFOTuHv7vuPF7xet6QkrEbFmQC9Wmov1BEtYazWy\nWcSM4vBEu4uzpvg62enwI1cuYwrlyHsgscuf1M+x0VEabdXO8EsnnuYzzVEAFpLbnEkk+GRDCbFv\nyWzwH6vneGfuVQCePZyn4mTY7iihc/5Rvdh63Kzua2+iejyi/xyaq/Tfe4+P5HbotRB6Ee6LO6HX\nerPGAj+O9x7DE9ebrBCylHraLrTQ9qwo0VbvSWbTsLXjL37O7h4ikcQcVnOEVBJsxxecEYEDx2rQ\nEGiCIpVSlszUCXX9C5cjiyUQaT8EGjoJ68iYNs8uwqqaE061qha8QSXkjtN2jXQ6UOD0YuFpyLLd\nVtfTY9q5cDmwaAj8TtakGrNXfmya/IrgcExbOS1Bal9iaw3dzkKr5DL4sjquzUHfVahNq+PWRBez\naiEtdf7iP74QGSueRh2RQXphdi5djfrcbgNfFqVTuDu7GFqBCi98d3M+rsOndj98Vxp6nCkaI0aM\nGPcJ7rmG7kW5mAvzIARCh7/9j//5c7yvUKYtlbbyqWYfO3aR7y/c0McjfFduj1c6qr2PppL8f800\n/11WrfifPTQxkbSkMr1//d3fGeU2hwZxq/UoL9+jpYX5QFDaGCjNy9ndi3rIQ+ZoWJMIXzvstbY3\nNiMrvNcmQIX2HR4iZlRUjFxZPcItH2vyes/W34fIZPywLadcVl56r/3ZtOprbVbat1ZV/+uQNFGt\nIxuabirkedenXmWrq/oiIRw+OPIyy906AC93RviL+iLf1/88AP/X/tvosw759sLLAGza/ZxNbvJ7\nFaVMvC13lT8+eJifHPmvAPzw9DuOtL+XHgtHEIhEEmN+GqEjdJzyAcbJGeQtZVF4WpKn9ZoTY0qj\nDvHUYV4bAjoAFC1xJ43JHB2h+ajimHcfTNDNS/JaqW+OCUaf77LxlNKo07uC3KZLeVH1a991l9ag\nQXZLWTvVOYPcuqT/Y8+oC9xGow5DWJZvARjDQyoiQnPe9vKKehZN4VhT49grtyJaLwTj2K3W1DiY\n1Br65aWI1gzaIvHmw8J8VHsfGICStqivXDvSfvPUHMKLcunxCVnzsxGrDKJzxLNifOrDdpS/IDSv\nzMWTXPoJdf/UegLrEFwdtdjtkyRqgvwtJR8aJwTNGZv8ddV3yYqkmws0eqMN3cEgdHbwSwZDv/3M\nUf+GN2dfXQrkQaeDs1/GHB5Wx43GUR783GlEVc1Xz2cR+f7UHHJDWUJuo6Fki6YNe61GYVl8uvu7\n31gO/auFNavCydAhWR7emV2l7prkDcWZfVvmgA1nm0/UFC88ljigKx1udBXv/WonwWxih11H8chn\nE5LfKj/BB0qKxv/1WlT49VI6EHSckcthjJZUyB3Q/LYzlBdMLP2OunkofbnLzbeqDjfb84w922L7\nu58CIFWWCAccHcTTf62D8cUrPpdqb2xipNO+ae0LsNFh3TgX9/ISlh7Mjna4+Oa0EIiZCSyPk0+n\nkJbp88z22jocVPz7IQTStn1B5d3PG5DW3AxSCIQXc5tMICtqYXXW1nl//0X2XXU8beX5jfIMVw9V\nv78lv8Ivj77E71TUYjiSqPFD/S+S1hPhz+pneCK9zvsHvghAS8JCZpvrdj7S92FKJBIGCTjlSvC7\nwX7Y3sM+UNyueXIWef1mkL8A3PzHTzH9fygu1BtTPlWwf6C44pADS9q2cqTqvuvlzcNhiM7WNnZW\nXSu9K3EtQWZXCa3DERMnbVD6kmpLedEkUXcZfUF976QMOnmJndYhnQZ0iqHYPy0M/fc21A+7+zCu\n3pPYr0Aq6Y9TV79P2TwM9VXg23A1/x2mCayZKaQWFO7mlup37Xey5maQB1VMnY/hVKsqTNLrq5Aw\nF5al4uiD1mOWhnC1A9e7Z3hOm2cWEC09xg51yOcDaj6zuRsRXM7FK5FwVevEGFJK/z25jQav/v1h\n+r+kJbgBzVGJndec+WVBY1JyoCMynYxL6VmTumJo6PQJGhMuqX212LaHHQpXTaymOr+bF6z+zFPM\nfGRJne/RZnXd11IeoVyPDVZAL14XLgdKxtCgclbr+S0KOag3cTwKdnQkcm3rxBjSdf02GAMDsM1d\nIaZcYsSIEeM+wT3V0IVhHDHFvEiLzx+e4PvyVf6ipbSdz9Qe5n8bfIGuVCvyK4dTPJHa480pZWqf\ntDJ8+jDDC20VKvjubJthq875jtZyt7Yxzy76jihrbgZZPgiScYYGfQ+7s7ODez3Q6A/+xgjFFZfM\nttKoN59MkWjYTD6ttNbDUhLXEpReCjQho+Oy/k2qLRUnychLCeRhYMq6rRauF5LlaYAhB5/Z3xeY\neSE6BwApaZzsp/0WbW4euHTzBn2vKI2s/dAEzZJF6Wll2pmGQCQSvvPOXl7BLJWCpKzZaZwr13wH\nk7OzG9HqfrvyAFeail76zYlnudYq8c6+S+o9VRep5Fd8KqwlHf6gvkhSqL4ZTtQYNTN8rqW0k3dl\nHH647yor2mN/JNKmx4Fszc9i37gVmLetFrgSI680fNHucuN/fwtSqyJDFxzSe7D+k8pa6l9yqI+Z\njPzmF4Jr6jBTACufVdaKptvclcaRCCnPYW2k00jbZvdBTU05AuHAzlvUsdmEnYdM8qvqvaX3JYfD\nJkZX67ECukVB+kCNaaspSJXdIGplbV05pweUwxnbwT6oYHpO08E+5MqqTwOYiydVQloIfpIaytqx\nToz5mcTO7l5ECzZLJSgNRMadkcshtGXq0XrHaZ7StrE3NrHSM/5n7vQoRlG/l3qT8qOj9Gsar3F6\niMxqw9cYuxMDHD41Q1JbgoljQkGd3T2f4rB7vrfGRkkcGNSntUa+BGZHYO1oJ2oGpCFxUup7qy5w\nLUjpoB03Cf2XBOVHVN/0XUhQOdclc1NZL4fTXfLXEsqSADBMzMF+OCYr25qdRiasyBy9+QtPYWfV\nvXNrgsPREwhbtc06BDsDbkI7/vtc+i6ZVE7PAvDAP1IBBN589J3e3ly5jSVwHO6pQJeuG2Sbaf7N\nm0wvNmb5pswzzGuv8y+ULnKlK/mBohqsqzYMmzlWbSV4/uneWX64/wX+3cGjADTcDf5O32Wue9mU\npRLOxSu+6W0vLfvefohSMNbUJLJWi6TDp3e7NMbVy85sSarTabrK+sPOCgZakkRD/X73zWmGz7fo\nv6rjwp2jFI85PITjZaV52YxaqMhqHadcxtIhWs7VKKdrLsxjNR2ErfqmPm6RKbvUHlCCoHjpgEQ1\nQ/VRJSiyf/Bc5HyPvwz3BeCHSZqJMdDp6fbKLZ7KXuVGa9g//8eGn+ZCR1EuZ7Pr/Mb+I6y3VTTD\n44Xr/I1cICD+Y/UhNpxDZi2PFsjze/VJvjO3fKTfQXGT5pkFn8+3vegfndrvbG5h9vfhappg/3vf\nRHZD0phUA2nnYYO+JUluU7XfSQoaU5Llf/Y2AOZ++hllznoZxZpOiAi6Mwug370fb40ON0smMbp6\nYjYhvSdpD2ghkgSzA7lNnSpvS6ozFkZCxz5bMHjJ5nBIXS9ZkbQHDJ8yMU/NQa2Bu6m5VM3D+hO4\nZyJLnTkoPAGvn8Xs1wvCaAnnxi3kI0phEHPjGI1goXYuXoGdncB3MzqM2K8EmcU98JQgwFeE5J4O\nxTx3Gsp1ym9VC6/hQPF6g+ZJdW1pgt2XQuTV4tQtWlhNl2RFUTBmKFoIQkrMfkC3maUSSPVeL/7i\nDOlNganl7WEJWiUHmVV9n76VpDvoMKgpmeo8NCYFrRNKICe3LaymwKpY/rvJ3Ezg6ijR0jMWhyOw\n+YPnVNd8/BWV9xKKPffCgqWUSiHSmeAkLJy0pE9PAycFmS2B1LKutuhg1QwGX1Hz10kYNCag77JX\njkI3wiszIQTCSgR+pFQK7pyu4eOec+geRMJCJLO+U+uBzDojZo4NWznfPlYdJmcU+eOOEizv71dF\nHB/WDp8OL7PvmjyUVfzrWneQpe4OKaH5S28y7EYFSDiY30u46HXWjD1/iJ2xaA0qIdN/rUttwiJZ\n0zGvmw7NERM7q7pP2HBYSlCbVi8otx6tU3IkXR7NbeaURi8cF7M06Me+G9ksIpNm6/vUxBy40qZT\ntMhfVYO9PdCP0ZF0Cqp93eEs3XzwKg/f+ziZP/qir9nZ128oLnIviDG2Zqd9LajXMThltplN7wZ9\nBnyxcdI//pv9z2NqNvXTjTMkhOALLSVcRq0KWSH4zKEa7Flxi7+sv5ULTU1mEnU2sbmLe3XZ59RB\nOTh9/n9yAkyD5Z88q97Ncw7SFEhdI6Q5LiifkyTLqu+HX7EZfV5Qm1DH6z/5FOP/PNDW6ctjdju+\ngAIdhqa5WsDntqWrkjr6r6j3eThkcDgs0EYjfdcd9t5s+Bx5+bRFdkvSHFHHxRWX6oxFdlvXcnEl\nCMPnvUWtpkIdtaDg8tKRFHdrbBRHC1Fpq5h8qmqO2KfGSSxv4ra8ejWGCv8r64Xh8hIOobh1et71\n3j7m6VOYni9lqD/iPA6PWS8EU+gQSnlrg+q3n/G/L1yvU5/NkV9WbWuNZUkctKguqPdqpw1SVQc7\np/MFGk0/FwVUEpTZ34cbWvCdnR1foOaXLJJVSflBr/YTpHZNnIa2nvTHTkr1fW4VKmdthp7X80JA\naxC/pkvtdBfrQAli7weJaqjujZ47fphlqaSsJUDMTfmyA8BZ28AujFLX8z+7KcEWtNRah1kzGLgA\n229VxyPPS9qDEmlp+6XbQaSCpClrYhx7dS2SdHm3iDn0GDFixLhP8Lpp6MbJGeTKmm/+vTX9Bf5z\nc4B3a1bkv8+vUnNtvi2rzLJf23+Enx9+lc/pxWrWatNnmPxeRWkfHyhd4EMHp/iRfh0Gde407JZ9\nT/GR4j6o8gIewqGHdtpk980Jn6fdeSiB0cU/bg9YGB1JZicw86szJqXzStPZfyDJYCghxVt5Pe5c\n3lg9NjTJ8yfguDh7+4x+XmnJ3eE80sQP8Sxeb2JWDslklam2/+YihZsdEhVlXnb6k75G5ff3yLCv\ncYmUqlpnLqjCZXZPdu7v187xg0UVLbTclXy88hgPZtWzPJxaZ8fJ8FhKabGl4gWeaw+x0lEa+la3\nyGpniHfmlUW175p8R//LzFtK8/pxFNctl3Uhs8kTyHIZkVXWCuUy5sQYsqa10NU1jGyWzI6ip3Yf\ntOi/6uJqWiO9Dbltl9qU0o42nzDJbAUZgV5Ym0dLOEvLETPag5/5GtIa1ReOn05en5aceMahNqWm\nTSdv4CagXdR0UU5yWBLkNqT+XpDZdTE0VdYuGrT7RYRDB3wO3DyzgLvSE0JpWUEFw1QK58q1IF19\nUvlFvPfoXLyirMEenl2GKkbaq2uRiB/n8lIk8elIeKFH7wypSByZ0tcaK2G2JblVRR8Z1UOKf7bF\nwbvUfOx/cZvW3CDZbT0GHYmTNjFbagy7tVq02FWo5EEYRklZ6KmKpHxOgqPaM3BBgJDUZtXvOv0u\nxVctOtrQy61LjLZBTVP+xevQGZBkttT53cMEqb3g+VqDUDvTJbekny9hqegjQ/h95YctX7qqInZ0\nvxlT4wy9aFBZUO9ZmgKzJem/qo4PFgXVOcHQS+rS20+4mA2D1pim+Q4qGIUChs6elunkkXDSu8Xr\nR7nsV8A0fZ6oZEr6jT0+sPM4AI/lrrOQ2OOEoSb6Pxg8z582c+w7ygnz9pTLB3ffxPv6VDx0xRW8\nNXMdR9vD7pVljMU5jLp2dmaikzgcByq7nUi8a3kxycAVm25WTdTKvEGiJsmFTOdmyaQ27ZWEVfzo\nzoNKwCYaUqXza8eevbmlTN2e7FDDy6rb2sZYmPMr1XnOFj8sMdev+DgvZ8CVNOf7SW8q03ropQpG\ntUntQcVzt4sm6W4nWgsmVKpUtttHYq/DbV1IbZIXamA36PDzw69SdtS9PtWcpGRV+YfriqP+1fEv\nsGfnaUk1lM5k1nm+Psfnm2qw/8TgNX6vcpLr1mjQ9+EY/kpdmZuaqzWHh9RCqHlsAGNokERdPXvh\nlktl1qTTpx2Re4JO3vDN5W5OkGhIn780MnpChoWFafo0hOh0sW+uqSqGRGOGjUIBt9Eks6/6Lvc5\nSXkxQW5DjYPWoKB4DeycukdmC4wutIY11ZWBdFnSHFbjxEkLhMuR6pI+Nndxm02/JIWztY0MlauQ\n7XYktM/oOkhAbgZcuzNUwEJJMVmr48yP437x5chtep1sYSepff2G3/fmqVmcpRvq8+1dpXy5OpX+\n+gqZQtqnVMR0luxWkfxNXQOo2SJz/laQvj40gDxRpD6thFbhxajC4RxUonHpOmxXFnW9IwfyKwbV\ns0ppqU/rUhIt3ffbQjmh9bioZpTQ339YvavdEZfixQSultfZdYnZCUKNq6ddEjsWzUk9Bz2nvQ4p\nFYmkrxD69JMuveGsrmM+MUrxumpLZVEi7CDOve+ihZPGryPTd8kkWZfUvGfoKZNgFAoYYyO4ntL3\nGnKF7m2USzqFeVJNdPviFeVU0IK0z0jzseoEj2RvAPDW1B6bjslvlNXgbLpJRhMVlttKSP1ye4Rf\nKF3kY1UV1/7q4Tj/ZPS8fy/Z7fipzQCyXEUkkr5HX9iOHwfqOWh955JQtR8yO2rwmF2L1L6NoWOf\nt9+SoZvDj27IbtlIU7D7oLp28UK0QJE5OhIRoEahoMrd5kJO2hCPay6epH52CEMnUSUPOqR2bLa/\nWU30wq0umbUG5XNKCA0+v4MzmKdd0BOxLSn/7bcx8NFn/Ovbt1YDrtZxwDAiccZegpnZ38e7s21A\nLU5zhvr7XFsN7M9VT2Mg+YEhde2EUPf0UvvPptf4lbHn+FhVcea/Vp7lXYULNF01c8zSoqppEi76\nRFCu2tTapKetGMODbH/rOHXtBD0cNnFTYLb15Fi22XnEou+qejfFZYmTEjTHtSaWl6x84Cnmflmp\nR26zqbRxrzbM4knMB07iXr1BL7wJtvWYesahC6rO/IFOHLIa0B2FoVd0XPqggXUImV31NImmSydv\n0JhQbTE6KtnIQ29xLJlJYeVzfvles5hX3LK2YkWxgLSChU5ahrI2QpEY8oVXCFz7wO4exkOK6xa2\ni7y2EilgJhtN0FE2ztXrkdIG7sqq70/wuHdzRHPoto21tkufoxN1Kg123zHO0AvKqjx4+zSpA5vk\nnhJ6rbEsrX6Toc/q913Mq9LJXg2gXA5nPXCSikIOdnZY+W5ltUpDRazkL2uJbEBmO4hqOXhAIpOS\n7E3VP41TXfYHDEzNsVt1E7MTlAYwbNh/s/RLA1g1A7MlsA7V+df+5ZMs/NxLQT2eUHKasTALENTA\nOX2Kw5LhO1jtoo1VMf0ol2RFsnPGof8VJR9SVRckaPJBJQKGSp2I8dGIlWXkclDnrhBz6DFixIhx\nn+Dehi222r55Z5477ZtzoDS9pzLX+S9NxTO/J1vjlu34ceg/MvAyB67NRztKQzibWeOL7S7v1Bp9\n2lBaytOHekWem8Hd2kHoAlX2zk6EH3RDlIhTPkC2naBsaUPSyRkcDqolt1MUWGOmz6GnyhKrAcUb\nQUhY+XSKvmtK+6rMJRghiKiRtXpkhXdrNaUd6Y0FzNOn/AqJAObmDnnXZeebFGeXuVmjeqaPwQs6\ndVVAfa7A4F+qkM+Dt5Sw0wLrUGv0VYfiH55XGWYaYqDPX/UjG0igTEjPWpDD6py/t/YkAKPJKoNW\nwy/O9fa7AjzBAAAd/UlEQVTiVf5WYY+f2noLANc6q1ScHL8y9iUAXmq3Od8RvFifBVQcO8C2o0xY\nZ2dHcY+aArLmZ5H7Zb/vnYOKXyrV67vBf/cM5t9S7anOG9hZyYDeuVaagpEXbSpzmtcuQKdf+rHg\niZpg6oNfIBx3ZM3P+lywqB8q68mzUHrLBCzMM6TDzZSGp8x1dQzJKuyf0TRfQpLdgLaOXsjuwGHJ\nwNTDJFmB/XMCb3sPadtHOGtrZioo06w1d68ss7u9Gyn/IF+8gCSgy0Q7o8JfQ/HMxkNnYEnnfiQs\nVfZVW0eyUlclLcJlYWv1oNhYKDfB5/tDtCGJBFJz0I0zI2T2bbpDyspMHdg0RhN0c17Ips3Q+Q3q\nj6lnSv/xF2FvP1JOw5qZ8ovEeX6n6V//MgC73/8g3bygMaXepMw61GcFmXVPhEkS+waHJ9T3yW2L\n7LrA1q4ZNwHlR2yGnlO/332bTf5KgrKu1pgqCxJ1OHizsm/O/OsyTqvl9wW1RlDkrHEY9YE5DrUF\nh9yKetaBL5s0JsDV4a6NcYFVM/0Y+kpWMviSQWswyBo2pidgV1kDzuUlMEx/l607lXfuxT1PLLIm\n1GCTtzYwZiZ9gfux6ipXWif44Iji+z7ZKDBk1hlPqIfMG2k+c5jnZ4bUTP63lSkut06Q14Gp785d\n4g8bY3xPTtkm/zyZQKRS0USmnm2iIvG3QvimbXNMkDqApOZlrUNJfr3DxlNKyKT3JMIVVObUcX1a\nkCpDWzM2mX01qLy4YqNQOJJ+flyVP786Wz6LTCUp/bmux55L03d+j52n1PfDX6qQrFo4fWq09r+4\nDUKw83ZFyQw8u4ndbuOEK8VBMHl6OFxjcQ77khb20xN8cPcBMobiN7+j8DK/W36Cvz34FwAMml2a\nbtJfQN+dW9F8u1r8/qj6CKfTG/yLE58D4Hcq07zcnGRA11EQlhXJD5BVFf/vO4yvrRxb1dJzguZv\nSqon8WPB2/0mRgcKt9S7rcybWBuC1rBXNjWaWOQnMXk1R3rqbDhLy5Fdb5yr12m9SwnMxqRAGhKp\nHWW5DUl+3aY9oGtwZCG/4bB/Wi8uOYGdDUq6lr5k05iygqSpfBZpmhGhJputwDmuk1zuVGvGyOUi\nCWPG4WHk9+LmJkyo6ztXrinlQTvXbY9L1zSkuXgSLBN0rXqRTPq5GcI0Ik46I51GttocnFP9OvTZ\nG1TfNoOVUItZ5tImmYvBFnROqY+dd05hahoxPzsNjuv3v1kqReaEtG313nRor5sI5iNA9nqC9rDr\nl+dIlQ2cFHSGdV38jkl7AHJrOvY7BZ1+i7rnJL2QULTpLe0UHYbDEy5mXYdBehUwtQ8gXIHSXWlG\nqlI6S8uMPDOKjs/A6ArchCS9rZ3leUlhGV+AO1moz0C3GKgZR8qGuw7SKw2xeBIuc1e494lFHiZG\nwZW+wK04OT448jKfbKhVcNoqM5dwGTPVS3ZklilrH1PoyA47z88OX+ZjVeUFP5nIs+NUUKILRLuD\n27Ovp31z1a8lE6k5sTAPO3ugV8Tiikuq4lIfV8f5NYeDk0kGLqv2758x6L/qUptRLyxZUZqhZemJ\nvqVfRA9P7HpFv7SG7K/4s5M4F68gsjrudGsHt9nk8L3KQewmBK3+gB1rjeVoDZrkb2mL4PEREk3J\n0Evq+jt/bZLhP7f8+x7JPEUNEnlT1wa5thLEXlsmj2Rv8Bc15eu41h3hp0pPU9Nexi8cTvF9+V26\nOnzk84cn+GzlLItZJYTfU/wy81aH8x31LI+mVxhLHLCna7l8wdbJGKE69tbUZMRhHKl/PjqCMAzK\nD6jvspuC4fMO24+q/ihoZdqzntJ7kvqUIHXgRS/IaGLRudORe8HRCAsv2c08swC7B2T2VD+bbYP6\ntCC7pTP+0oLKfAJbh7APXASjLclpDb58FqSQZHQ2Y23aIrsmgqSppWUVlRJawEQ2HRFs4fK9XiSW\nl5CGK3E2Nn3fiHPjFrLdxnhQdZa8vIycGkU0Q8lFl5d8rdOrYeNbhqdPqcXWs+zGhnH1Aii70X5y\nWy263/wmBj+txpUzXqJwad9PDGo8OkP2yq4q+QsYBw2SjZxfn9xZXY/U+/Yctb6zumsj98o43nt7\nxxi1WcGwMgTZe8jFago//rx5QqrEoaq2CA6EcpxrJbj8sEPxVYvmuF7ok9AZdDmc0Fm8NZPMhoG3\n5bFnNYVryXjOYiOXxdne8aOV3L19tp+UoH1sfUvKWetZB6kD5SjXKTJYdYOBVyXVWc0m6MiqXnnh\no3J3G9NAzKHHiBEjxn2De0u5WGZQplUIZKfjaxeGeI4PHUzx/QVV1+DLnSKdbovHUxn//Kyw+WRD\nhfr97PBlfnlvgTNppWW+2O5w4Ba40lVc7XFbuiElUqffm6MjflaaXN/Sm/XqEKidaXYfSgalOfMW\n+VUXW2ehZTclzRGDgVfVktscNUmVoatjYBN1vRRrjd/jSf2KdGvramcfr5i/9iv00jCZP1IVC81z\np5Fn+8mt6/MzJgNfrrL9pKKIRp7dRxy2cfqUNlH61PLx5XpDO+kI2wHNmzt7+0Hm22Gb6+0gxHDH\nLvBre+/gXQUVl+6VJ357QbV53tpnL5/nwZRq+583TjNUOM+TafXsP7r2FN818BIP6Bo8auvZQAs+\nEveNpgZ0KWORTGLfWmX+p5WmVnvfk9SmTLL6csmai3CDDMHyA9C3JEmXlbY09VuXoFTyd4c/LgBM\npNOA0iy9zYFBxRuboyMcDumd601BshKEnw2/bFNesPCSapMNl8Nhi3a/podWJEYX8uvKfN95OEGy\nBu5+UCExsqvW1GS0JIG2JsJx6+apOb90g19vZFtbFCMltQvY+VeDa+zX/NA/f7MQb6euHurNubwU\n5fTL5chmHGErxpoY52A0QeeblFaZPLBxMjkyq/r3hqD8+Ci5DWUlO8kCxUsHCJ2t7GYyyN4NIgwT\nobNgyaTVZtjaqt191CW/bHKos3D7X1XctMepO1kXq2mhSwzRLUBmR1CbVW986AWTbj6op1Kfc+i/\nYNE84UUgCQ5HXWQy2BZQ9BX9sen5JQBkuwO1WqT/jEMDJ6fmfbvPAqEoOID0vvK7HI56tCHsvUnQ\nt6QtPS9bNqSZG4UChhdKe5cbYsC9jkOX0t+x3L68hFks+uU3/07xGuc7Jr+vTf1du8CDmZt8UU+0\n5w/nqTtpmjo26LtzF/ihvi/x+Zbq6E2nyIRZ4b80FiO37BVqXupy2BElbftI+rvVgKGLijPceShN\nc8SgdF6FYLUHEuydsdg/q4REoqZC1Yqax916a4rp9WjRKbO/zxfwIp+LTFyvCJSxoE2uV5f8Ot6g\nHFHtp97m84H1ySR2ro/CmjJZmzNF6uOWqucMyGJRmeqX9Z6SOr7XD8EaGFA8sI4ptmankbqmthd/\nW9X24kiyyiPZFa52FA/7rdkrfPawn6erKhTuaSBvtvmevHrW54XDJyqP+u/p/5x4jh9de4IzOb1N\nnrezVE+NHaOoOe5dRX15eQHC2yFdC7X+l8sgB9h7s14MUwaJuvTT608841JetBj6bV3P5txpWN8G\nr8zCwVHz1db1YkBv/6V5Y7NYxNnaJllTiTvpfZvmiOVz6DsPWRRvSKqzOvmmbZBoSDw7X1qCxgg0\nJtQiOPKXNltvtYKdqxZPgpQIbaqHE90A3MuKdvLmiDUxjt3ji4kUFiuXj+xAhG2DDgcUgwOYQkS3\nLNzeC2rD652k/NLDpqk4dUDsHUR8TvbaOkN/alPVAl24kF2uIrSfys4WGHx2k/pZtXjkXt3h4NFR\nQCkVhU88i0ilME9o5cFx1R68OsFMWmZkW7gz/2qT1e+eoHZWLRBDzyXoDDgUllX77IxBp0/6Tloh\noT3q0ndF7zE6D92RDuaBLkTWNhCO9Ff49qBL/qbB5P+rQzYPWxgjQ0HYc6UW4dEBP8FLWBbDL0ma\no9p30qfi4VO7HtUmMGyVeAbQKkmMtmD3MdVXgx9R1/Nqw3gC/rUIcg9fUaALIaaAjwGjqMf/sJTy\nXwshBoFPALPADeBvSinvvIOwZfnaBACmiTGr+MCm7JI1XGquIrFOpbb4YuMk35xX2sa3Zq9wLpnh\npq71su0IrttZJix1yzGzye/sP8WDWb2phKcV68w7L/PKm4xHYAbsU3UmQaIhaYwpoWQ1JfkNh8qc\nalvftRbJikVeF2WqTpt0CoL6pI5DX3YRtWaQkZhIqM2WNR9oNJqIVArDy17UA8Vrqzk8rOpyhJo3\n+uk1v1778KWbiL5CxApJhX7rVKuYZrD5QK/m75TLmGPDQS3tcjngZff2udYqMZ9RnGbBaPFU5hZX\nu6qtH9l7O4aQfE//iwBc7Yyx3hlg01ET61tyl3kwmeZPm0qI/ezWgzyYv0XFURPV21DX7uEJI4Wa\n8jmE3r/RF1ja/+JcuMxAdZLCJwKt/sYH38bYHyz7fRnYdKFCaP1K2/GEY+9GBl6BJPvGzUitdmty\ngkOdGNTNC3JbDmZXVxQcM8js2hi2evbKvMHAZZfmCV0R8CoM7Eha2oG7+YRJ3zUZFIkzDbXo67ZY\nkxPYaxuBP8O2o7x1oxGxaMzRESgGdeadq9cRna6fOUoygVzfBqGdc1qJ8aNYVjciERSeteDHW4e3\nTVw8idnt+gu+NTaKvbVN33O6326tRiKJ8peuIk/Nkf2zS35/F/9wHVNnftroBDdvm8epSVUPvqbr\n0HgKl1cR9dYaifo4Q896Baxg6CUDqVOCq4sOiYpJa0z7r5oGTp+NrTduLl6T1GSQ/W1VBJXTri/Q\nEzVBN09kMXSWlqMcupc121fEOaj4Frc5UqLv48+iYyICh6mXHCdd9U68Mdy716+XROVl4YKSDd7G\n57fZe/Q43A2HbgP/UEp5FngS+BEhxFngp4HPSikXgM/q4xgxYsSI8TrhK2roUsoNYEP/vyaEuARM\nAO8FvlX/7N+jrO+fuuPFhAgqzVmWv0s5qNK4Jk1+YlCtkL+yf5IPlL7MP9p+GMDPAr3YUZTNy61J\nviX3Ko/rVe1PmwM8mL3FelfvErS8orTy3tVNe93DO4hbE+MRLXb4w88ovtkLudJaYj4UXjby+eCS\nxalJZCrhr7zW1GTUXOp0EdPjiB1Fthr9fcjm4RETzm9rs6n4Ti9KJcybahi2HVQs7Noq3E5zo7JS\njexkY81MgRCBNjQzhXP9ZqSGh/f8IpHkfxl6moKhtImPHjxGQRg811C+jq40+Y6+l3G0LlCyqswn\nt3n2UGmFZTtHK3+Rz9bUe/vl0Zf40MEUf7dPhaM+Pfs+nNWNSLSRUSz4z24uzOMu3/RpCGNuGufq\ndd+6MUdHIhEBbO9x6kPL2F7NHl3FMpzObs3N+DSKmB6HS1eD81Hp/154HDs7vm9DnWySquj4/rqL\ncKCrywm0SpJ9kfCrLwoHrJZk/PPqXuvfYgGCZFnztI4q++q/51eXItu4uQcVzIE+hOZOZaUKrgzq\n0FSqyFTCt/ycrW3Mdlv7ADSFUqnheFsfzk5DjyVnFAoYXl2WWh16YpzNs4t+VU5neyfIni5XYaAP\noUtpuNUa5sJ8pA7QkWiqhIVR0hTr9RsYhUJQ67tYRAz2+207bps2kUhieiGXq+sM/c4z7P3PquRE\ndR7MQ0G3L4hSSW8L31qqzbmk1pLUTqq+zWyY4Eo/KsbOSoyOKt3gXW/250NVOVFWvWdFG6OlIIfl\nsKfGimVGKqr681q/VyOdRtiOH1svLAsjnwvCSS9cjvSdkcupnIHXoJn7ffZa9hQVQswCnwPeBNyU\nUvbrzwVQ9o5vh77MCfmE+04AtYO9GxSI8mt7eHsL1pqRl6zM0fUgAaRYRAwPRrjwsBBmZOjYHeLD\nMb/+eT1ha9bstKrP7fWNEDhrG5iaBrA3tyIp0t4O4JGwo/CC4e0pGuI+jWzWf2HW2Ciy20VoU7x3\ncJtnFnCvLgfOuuUVFU+sa0kghBJynoC3HdzlmxiLQdEmIOLgOnJ9L/ZYCP76C9s8lVWc+JNpk7rb\n4sMHqnzt9xbOc7U7wCstRdG8J/8KNZngTdrZ9FsHD/BdhZf5k7qqKz2b3GXPzvNURr3fH59Vxbn8\nMgQAur403HnvVODoHpi95WZnp9VGEaEF1SyV/Mnl7O2DYWJpqs/uMe3De2T23stL1ffHrA737N2b\n1ksIotMFxwlKQJw+BfsVpH7v3l6SXvuPFJAzTIxk4kiRJr+Ofr0BlhU49237aFEnIYI6PRubRzl2\n/ZxwfJnWsCMdolsHRrZJ9H7v0T0A1foRpaX3GcJ9Zi+vRBONjpljHrZ+7ClaJUl7QjmcS59P4KSC\n8FUAKQQHD6rFdeDLJq4l/C3nnLSgNu+S2lMnTP6TL0T7Ts/fiOLh0S+HLcwHTvr0scjnjow5IFKe\n28jlfHlgFAvH1qD3OHT3oIIQwvcjmafm+NTVX7mrPUXvOmxRCJEH/hPw41LKyI7IUq0Kx64MQoj3\nCyFeEEK80LFf+4oTI0aMGDHuDneloQshEsAfA5+SUv4r/dll4FullBtCiBPA01LK03e6Tl9qVL5t\n8H8A1Ia2stvxNQC33sAcHw1M39VNnGo1qhEszIMOO2R0GLmy5mtzYY03DL+wUSGPrNQQISdSr7Mw\nrDWEy9nam1tRZ1SphDCNqCMvtJVarzZhTYzj7O4Fu7BPjKtKejqxwshmwXWDYvrDQ0pr7d31yNN+\nqnXVf17mq+scry15proXJeFlKA71w60NldwFqlDXheNT0bxKcLeDUSiA4/gZhebEGDKZOKIF+n1x\nYgwsy+9LI5tF5HJQ0lXteiyzyHm6rfbaemQne5FKKS2JwLrptUb85JvLS0ffzzE70kf6ILRp9JHv\nikV/CzlZb+CUFW0CyhowSyU/PM9ttdSWb9oS86J9jiSSaHiUg7O26T+LNTUZ7C61th7ZBNqnO3qs\nDa/9dG210463UbunVXpOWS/KJUTF+c/Zs1UgaHpLl2hga8cvAwtBYbNwv5ujIzCoKZzN3SPjyjw1\nF9n43JqZwtaRP+ZgP+5BxZ8zIpHk5k89Rqukqbku9F8WHOhU/sGXBc1R4ScKmW1oTNv0X1Qss5NS\nm79Pf0DRLEY6jXRczJFh//69JajDMHI5jIKWJdkMNA9xdMVQ4+TMseyAN3+F7WAvrwRlkLNZxSh4\nZZQXTyKarYjG/xn5+3elod9NlIsAfge45AlzjU8C/xPwz/TfP/pK15KdbsQEMxdPImpKIMtyORqD\ne3YRLlaDiScElCuBkNN/I17o49qvO10eVFSdEB3tEC4n6wktL4oEwJif8XcQAl2tMPRCZMLCHFIU\njDhsR/ZAlK0gMw9C8b7e1ma1uioT65mTehC5mk8U+Zwy8XpMYW/CWpMTkYFmDg1GhLmRzeK22oEg\nn5vBWd8M7letqTKtJ3SZ1ktBVcpeQRyu53EcvPfjZ81t70ZS+T0h6G97lkr6uQCg+GRzYgypKR/X\ntv0MRr/PXCegJSbGEZYVSXe3b9z0fTE+LTJ5AghFboSq14WFuVEoBCYx0XHkLZKiGtRKd/f2I4qD\nKOSRZV2HphoN/QTUbjRTqm/Mah36ChGe2V1Z8wUoO2Ul5HUMvlOtqnHg1QTqdpCFrC8sjGwWZ21D\n8bGEFqYQd+u2Wsgbq/6ziVQKZ0NTQIMDkX13OWypsdUJqjf6MfCXrmJNjPvb57n1BjKbPiLkxZTq\nd3PDOqoI9BeD8OFUyn9X/rn1Jm41NN9tx38WUcgjd/f89ySmx5n7+BrOLSX0Nn7scQ4WJYmKjkjK\nKiHvlVlOVgTJsom3s+L0LypB7kX8eHPML+O8MO8L3l54yl14rBiFgj9X/HpVWlmVh4cqO1gvvJ7f\nIRLlFuoroRfacAmKu8VX1NCFEO8APg+8DH5k0s8CzwG/B0wDK6iwxf1jL6JRFIPyCfEuIEjv7tUG\nbsfzHsetmkODvrOLsVJEGHmra1jrNtJpDB33ai+vBI4xIY59eeHt6iCqbUBQe0U2Gnjp+5H2hepa\nHwefc79+EzOfAz0Z5A01WMJCpjdOPny+sJ1IXQwvrruXp/MWCKM0HCmn616/eVtNxIPHC/t9FHIc\nmgvzQaq+l7yiBbqoNyMLtbAsjNkpREvdz99qrodT9a+thWo4RteaGPfrj7jbu4i5KUS96Z9vZLMI\n7XCSmzv+5smAWlDqDT8fwp98t0m7Ns8s+Cnd7B+AbfsCz202lVbrlYDN56ObNoyOQKcb1Lnu70MU\nC0esgfC9jXQaMaf6whd+WugYAwORhdsTsOE62lhWkEbf36f43nHtfLu1FhWgiSRGPhe1bkK+n16E\nx2Cvxt5r9ZhDgyoxx9sYvViEEyMR3tnd2fULjznlspIFGzv+sX8ewFgJ0bWPFbCRvtR+JFGucvGX\nplj84RdU+7JZFVevrVqRzaiyE54/IpWMbJzuj8vece/dZ3TEr7UjshnllP8KsvQ4y8drc7gvj7MM\nvm4aupTyz/ErIhzBu77S+TFixIgR497gNUW5fK3oS4/JxzvfFPnMN6HGR1VBHh1W6GnBPs+kN5D1\neWetEXjfy3ZbaTfa1PY0rfD3YRzhBIXA8MqUNptKu9EhWubwELLduSOX3MuFWpMTgYnlFfrR/KIY\nK6mNC3qzWL3MMyEwToxi601pjVwWul1czxR2HZVlp5M0kPJIGvexnHqPheHByOVUOF+oLeHvjOHB\no/4GzcO6e+WIZuY9h88vOu4d+827lrflnLO3fyQSw/NZAEesDm9jgF7utncDcI/ycfb2IyFm1syU\n8kfoQm7mQP/Re2ir0RwexN7aCeijdlslcYUokiPP1lPaoNcnYZ6aC6ofeqGjehx4qfz+b8+dvq2v\nw7/fMVst+ueHd7Di6Bjxd9Hq0+O00/WtD2FZGAtzR8ZH5PpnF3GvqX6V7fbxVvVtaATzzILKWg3R\nVebpU/5YNbJZjIF+ZJ8eV9t76ho9/gLPEnN2949ouWGKxxwdwdnZC/wMAwMwPBBkpwqhaA+PZu3a\noDfT7u3fcDQOHC1P7WVq325TejgaaWeeO+23RWbTfOqVX7orDf2eCnQhRI27LgR532MY2H29G/FX\nBHFfBIj7IkDcFwFmpJSlr/Sje72n6OW7WWXeCBBCvBD3hULcFwHivggQ98VrR1w+N0aMGDHuE8QC\nPUaMGDHuE9xrgf7he3y/v8qI+yJA3BcB4r4IEPfFa8Q9dYrGiBEjRoxvHGLKJUaMGDHuE9wzgS6E\neLcQ4rIQYkkI8YarnS6EuCGEeFkI8ZIQ4gX92aAQ4tNCiKv678Dr3c5vBIQQHxFCbAshXgl9duyz\nC4Vf1+PkvBDiLa9fy7/+uE1f/KIQYk2PjZeEEN8Z+u5ndF9cFkJ8x+vT6m8MhBBTQoj/KoS4KIS4\nIIT4+/rzN+TY+Hrgngh0IYQJfAh4D3AW+AG9ScYbDX9NSvlwKBTrjbJJyEeBd/d8drtnfw+woP+9\nH/g396iN9wof5WhfAPyqHhsPSyn/BEDPkfcB5/Q5v6nn0v2C17p5zv0+Nr5m3CsN/XFgSUp5XUrZ\nAX4XtUHGGx3vRW0Ogv77Pa9jW75hkFJ+Duit83O7Z38v8DGp8CzQr6t53he4TV/cDu8FfldK2ZZS\nLgNLqLl0X0BKuSGl/Ev9/xoQ3jznDTc2vh64VwJ9Agjnjq/qz95IkMCfCiFeFEK8X382qneEAthE\n7dv6RsHtnv2NOlZ+VNMIHwlRb2+YvtCb5zyCKvoXj42vErFT9N7hHVLKt6DMxh8RQnxz+Ms7bRJy\nv+ON/Owa/wY4CTyM2u7xX76+zbm3+Go3z4lxFPdKoK8BU6HjSf3ZGwZSyjX9dxv4f1Cm85ZnMuq/\nx9fZvT9xu2d/w40VKeWWlNKRUrrAvyWgVe77vtCb5/wn4ONSyj/QH8dj46vEvRLozwMLQog5IUQS\n5ej55D269+sOIUROCFHw/g98O/AKwSYhcJebhNxHuN2zfxL4IR3R8CRQCZnf9yV6eODvRY0NUH3x\nPiFESggxh3IGfvFet+8bhbvYPAfe4GPjNUNKeU/+Ad8JXAGuAT93r+77V+EfMA98Wf+74D0/MITy\n4l8FPgMMvt5t/QY9//+NohK6KN7zh2/37Kja+x/S4+Rl4LHXu/33oC/+g37W8yihdSL0+5/TfXEZ\neM/r3f6vc1+8A0WnnAde0v++8406Nr4e/+JM0RgxYsS4TxA7RWPEiBHjPkEs0GPEiBHjPkEs0GPE\niBHjPkEs0GPEiBHjPkEs0GPEiBHjPkEs0GPEiBHjPkEs0GPEiBHjPkEs0GPEiBHjPsH/D1OtQZc1\n0V+FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104fad150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = img_handler.get_images(list_name=\"test\", index_low=0, index_high=9)\n",
    "print \"Object type:\", images\n",
    "print \"images.imgs:\", type(images.imgs), \";\", images.imgs.shape\n",
    "print \"images.labels:\", type(images.labels), \";\", images.labels.shape\n",
    "\n",
    "plt.imshow(np.concatenate(images.imgs[..., 0], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initializing the DNN\n",
    "The architecture of the DNN is defined in a layout file that has the structure of a list of python dictionaries, e.g.:\n",
    "\n",
    "    [{name: \"A\", type: \"conv\", ...},\n",
    "     {name: \"B\", type: \"conv\", ...}, \n",
    "     {name: \"C\", type: \"fc\", ...}]\n",
    "\n",
    "Each dictionary corresponds to a single node, or layer, in the DNN. The type of node is defined via the 'type' key. The currently supported nodes and the required dictionary keys are:\n",
    "\n",
    "* Input Layer ('input')\n",
    "    * In tensorflow terminology, this is a *placeholder* into which data is loaded at run-time.\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'input'\n",
    "        * data_z_index: (list or integer) The input data can be split along the z-axis/channels. This list (or integer) indicates which z-layers/channels of the input data should be read into this particular input node. If this is missing, the class assumes all layers are to be used as input. This is useful, for example, if each color channel of an image should be trained on independently rather than together with the other channels. Convolutions on 3D images, at least in Tensorflow, are across all dimensions simultaneously. If the individual channels are independent, however (e.g. in biological images with N staining channels), then this can lead to a significant reduction in classification accuracy, see [here](http://biorxiv.org/content/early/2016/11/02/085118). \n",
    "* Convolution ('conv')\n",
    "    * A standard tensorflow convolution across the input data\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'conv'\n",
    "        * input: (str) The name of the input node for this convolution, i.e. the node that feeds its output into the convolution\n",
    "        * size: (tuple of integers) The size of the convolutional kernel across the spatial dimensions. The convolution is performed across all channels at once, there is currently no provision for having a convolution across, for example, two of three channels at a time. This might be a useful addition later on if, for example, one were to feed time series data into the DNN\n",
    "        * n_kernels: (integer) The number of independent kernels to use. A convolution layer with N kernels outputs a feature map with N channels.\n",
    "        * stride: (tuple of integers) The size of the steps to take for the convolution, i.e. a stride of N means that the convolution is only performed every N-th pixel (e.g. to avoid overlaps)\n",
    "        * padding: (str) Must have a value permitted by tensorflow (currently only \"VALID\", i.e. the convolution is only performed when the kernel is completely inside the image, or \"SAME\", the outside of the image is padded with the nearest pixel value to allow a convolution even at the border)\n",
    "* Maximum Pooling ('maxpool')\n",
    "    * A maximum pooling operation that returns only the largest value in a given window\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'maxpool'\n",
    "        * input: (str) The name of the input node for the maximum pooling, i.e. the node that feeds its output into the maximum pooling\n",
    "        * size: (tuple of integers) The size of the maxpool kernel across the spatial dimensions. Maximum pooling is performed channel-wise. At the time of development, tensorflow did not support pooling across channels\n",
    "        * stride: (tuple of integers) The size of the steps to take for the maxpool operation, i.e. a stride of N means that the pooling is only performed every N-th pixel (e.g. to avoid overlaps)\n",
    "        * padding: (str) Must have a value permitted by tensorflow (currently only \"VALID\", i.e. the pooling is only performed when the kernel is completely inside the image, or \"SAME\", the outside of the image is padded with the nearest pixel value to allow pooling even at the border)\n",
    "* Fully Connected ('fc')\n",
    "    * Fully connected layers are used in classification but not segmentation. To allow for flexibility between the two tasks, the FC-functionality is implemented as a convolution with the size of the input and a \"VALID\" padding, so that the output is a (1 x 1 x K) feature vector, where K is the number of 'pseudo-convolutional' kernels used.\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'fc'\n",
    "        * input: (str) The name of the input node for the fc layer, i.e. the node that feeds its output into the fc layer\n",
    "        * **[optional]** n_kernels: (integer) If n_kernels is given as a dictionary key, then the layer will consist of the corresponding number of convolutions/fully connected features. If this key is missing, however, the class will automatically create as many output features as there are categories in the training data so that the output can be normalized and interpreted as a probability for each category. The last operational layer of each network (before the definition of the loss function) should be a fully connected layer without a *n_kernels' parameter\n",
    "* Concatenation ('concat')\n",
    "    * Combines multiple input tensors along the channel axis. E.g. if two feature arrays, each with 128 channels, are concatenated, then this layer outputs a feature array with 256 channels\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'concat'\n",
    "        * input: (tuple of str) A list of names of input nodes for the concatenation layer. This **must** be an iterable object (list, tuple, numpy array), even if, for whatever reason, only a single layer serves as input.\n",
    "* Inception ('inception')\n",
    "    * A compound block inspired by Google's [inception architecture](https://github.com/tensorflow/models/tree/master/inception). A block consists of N independent branches, each with a varying kernel size, that are concatenated to a single feature array at the end. An example of such a block is shown here: \n",
    "    \n",
    "    ![InceptionBlock](inception_module.jpg)\n",
    "    \n",
    "    The pooling and convolutions are performed in a way that results in the output feature array having the same spatial dimensions as the input layer. To further reduce computational time, NxN convolutions are split into an Nx1 and 1xN convolution ([Separable Convolutions](https://arxiv.org/abs/1610.02357)).\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'inception'\n",
    "        * input: (str) The name of the input node for the inception layer, i.e. the node that feeds its output into the inception layer\n",
    "        * branches: (tuple of ints) The 1x1 convolution as well as the pooling+1x1 convolution branches are always created. Additional branches, i.e. kernel sizes, are defined via this parameter. For example, to create an inception module with the branches shown in the image above, branches must be set to (3, 5). Duplicate kernel sizes are ignored.\n",
    "        * n_kernels: (int) The 1x1 convolution in each branch is used to \"downsample\" the input feature array in order to optimize computation time. This parameter determines how many feature maps each branch outputs. The final output of the inception is a concatenation of all branch outputs, so accordingly it has n_kernels * number of branches feature maps.\n",
    "* Loss Function / Optimizer ('cross_entropy' / 'rmse')\n",
    "    * This is not an operational node but the loss function that will be used by the gradient descent algorithm when training the algorithm. Even if the DNN will only be used for application and not training, a loss function must still be defined (but will then be ignored). There may be only one loss function defined in the layout file. Currently permitted loss functions are the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) and the root mean square error.\n",
    "    * Required / Supported keys:\n",
    "        * name: (str) A unique name for the node\n",
    "        * type: 'cross_entropy' or 'rmse'\n",
    "        * input: (str) The name of the input node for the loss function, i.e. the last operational layer of the network (usually the unnormalized probability vector for each category)\n",
    "        * optimizer_type: This must be a native tensorflow optimizer (defaults to tensorflow.train.AdamOptimizer)\n",
    "\n",
    "An example of a layout file is given below. This layout takes input data with three channels and passes each channel through an independent node chain before concatenating the results:\n",
    "\n",
    "    {\"name\": \"data_ch1\", \"type\": \"input\", \"data_z_index\": [0]},\n",
    "    {\"name\": \"data_ch2\", \"type\": \"input\", \"data_z_index\": [1]},\n",
    "    {\"name\": \"data_ch3\", \"type\": \"input\", \"data_z_index\": [2]},\n",
    "    {\"name\": \"conv1_ch1\", \"type\": \"conv\", \"n_kernels\": 64, \"size\": (3, 3), \n",
    "     \"stride\": (1, 1), \"padding\": \"VALID\", \"input\": \"data_ch1\"},\n",
    "    {\"name\": \"conv1_ch2\", \"type\": \"conv\", \"n_kernels\": 64, \"size\": (3, 3),\n",
    "     \"stride\": (1, 1), \"padding\": \"VALID\", \"input\": \"data_ch2\"},\n",
    "    {\"name\": \"conv1_ch3\", \"type\": \"conv\", \"n_kernels\": 64, \"size\": (3, 3),\n",
    "     \"stride\": (1, 1), \"padding\": \"VALID\", \"input\": \"data_ch3\"},\n",
    "    {\"name\": \"incep1_ch1\", \"type\": \"inception\", \"input\": \"conv1_ch1\", \n",
    "     \"n_kernels\": 32, \"branches\": (3, 5, 7)},\n",
    "    {\"name\": \"incep1_ch2\", \"type\": \"inception\", \"input\": \"conv1_ch2\", \n",
    "     \"n_kernels\": 32, \"branches\": (3, 5, 7)},\n",
    "    {\"name\": \"incep1_ch3\", \"type\": \"inception\", \"input\": \"conv1_ch3\", \n",
    "     \"n_kernels\": 32, \"branches\": (3, 5, 7)},\n",
    "    {\"name\": \"maxpool_after_incep1_ch1\", \"type\": \"maxpool\", \"size\": (3, 3), \n",
    "     \"stride\": (2, 2), \"padding\": \"SAME\", \"input\": \"incep1_ch1\"},\n",
    "    {\"name\": \"maxpool_after_incep1_ch2\", \"type\": \"maxpool\", \"size\": (3, 3), \n",
    "     \"stride\": (2, 2), \"padding\": \"SAME\", \"input\": \"incep1_ch2\"},\n",
    "    {\"name\": \"maxpool_after_incep1_ch3\", \"type\": \"maxpool\", \"size\": (3, 3), \n",
    "     \"stride\": (2, 2), \"padding\": \"SAME\", \"input\": \"incep1_ch3\"},\n",
    "    {\"name\": \"fc_ch1\", \"type\": \"fc\", \"n_kernels\": 32, \n",
    "     \"input\": \"maxpool_after_incep1_ch1\"},\n",
    "    {\"name\": \"fc_ch2\", \"type\": \"fc\", \"n_kernels\": 32, \n",
    "     \"input\": \"maxpool_after_incep1_ch2\"},\n",
    "    {\"name\": \"fc_ch3\", \"type\": \"fc\", \"n_kernels\": 32, \n",
    "     \"input\": \"maxpool_after_incep1_ch3\"},\n",
    "    {\"name\": \"concat_channels\", \"type\": \"concat\", \n",
    "     \"input\": [\"fc_ch1\", \"fc_ch2\", \"fc_ch3\"]},\n",
    "    {\"name\": \"output\", \"type\": \"fc\", \"input\": \"concat_channels\"},\n",
    "    {\"name\": \"CrossEntropy\", \"type\": \"cross_entropy\", \"input\": \"output\"}\n",
    "\n",
    "For the training data, however, a much simpler DNN will suffice to classify the images. The structure of this network is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dnn_layout = [{\"name\": \"input\", \"type\": \"input\"},\n",
    "             {\"name\": \"conv1\", \"type\": \"conv\", \"n_kernels\": 32, \"size\": (3, 3), \n",
    "              \"stride\": (1, 1), \"padding\" : \"VALID\", \"input\": \"input\"}, \n",
    "             {\"name\": \"conv2\", \"type\": \"conv\", \"n_kernels\": 32, \"size\": (3, 3), \n",
    "              \"stride\": (1, 1), \"padding\" : \"VALID\", \"input\": \"conv1\"},\n",
    "             {\"name\": \"maxpool1\", \"type\": \"maxpool\", \"size\": (3, 3), \n",
    "              \"stride\": (2, 2), \"padding\" : \"VALID\", \"input\": \"conv2\"},\n",
    "             {\"name\": \"conv3\", \"type\": \"conv\", \"n_kernels\": 64, \"size\": (3, 3), \n",
    "              \"stride\": (1, 1), \"padding\" : \"VALID\", \"input\": \"maxpool1\"},\n",
    "             {\"name\": \"conv4\", \"type\": \"conv\", \"n_kernels\": 64, \"size\": (3, 3), \n",
    "              \"stride\": (1, 1), \"padding\" : \"VALID\", \"input\": \"conv3\"},\n",
    "             {\"name\": \"maxpool2\", \"type\": \"maxpool\", \"size\": (3, 3), \n",
    "              \"stride\": (2, 2), \"padding\" : \"VALID\", \"input\": \"conv4\"},\n",
    "             {\"name\": \"fc1\", \"type\": \"fc\", \"input\": \"maxpool2\"},\n",
    "             {\"name\": \"CrossEntropy\", \"type\": \"cross_entropy\", \"input\": \"fc1\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The network wrapper is initialized via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainer = DNNSwift.DNN(\n",
    "    img_dims=img_handler.get_image_dims(), \n",
    "    labels=img_handler.get_image_groups(), \n",
    "    layer_params=dnn_layout, \n",
    "    basedir=\"dnn_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The required parameters are as follows:\n",
    "* img_dims: The dimension of the training images (spatial, spatial, channels), e.g. (21, 21, 3)\n",
    "* labels: The same label dictionary used for the ImageHandler class\n",
    "* layer_params: The list of dictionaries defining the network structure\n",
    "* basedir: The base directory into which training and validation results should be written (note that this directory must exist, this package does not create it).\n",
    "\n",
    "Optional parameters may also include:\n",
    "* weights (defaults to None): A dictionary of values with which to initialize the network (see further below for more information on this). If not explicitly defined (or defined as 'None') then random weights are chosen.\n",
    "* learning_rate (defaults to 1e-3): The initial learning rate of the DNN (note that depending on the optimizer chosen, the learning rate will be changed throughout the training process\n",
    "\n",
    "An image of the network structure can be created via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainer.print_structure(filename=\"DNN_Structure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The *filename* is the relative path under the objects *basedir* folder. The output type is defined by the file ending, e.g. \\*.png outputs an image. The full list of permitted output types can be found at the [PyGraphViz](https://pygraphviz.github.io/) homepage.\n",
    "\n",
    "## Training the Network\n",
    "Running the network training after initialization is as simple as executing the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainer.train_network(\n",
    "    batch_size=128, \n",
    "    image_handler=img_handler, \n",
    "    num_epochs=1,\n",
    "    subdir=\"weights\", \n",
    "    logfile=\"dnn_output/DNN_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The required parameters are as follows:\n",
    "* batch_size: Stochastic gradient descent methods train data in batches, meaning a number of images are put through the network and their respective losses are averaged prior to parameter adjustment. This significantly decreases the time required to train a network to convergence and also helps avoid local minima.\n",
    "* image_handler: The ImageHandler object instantiated earlier, which holds the training data\n",
    "* num_epochs: The number of iterations over the training data. For training neural networks via stochastic gradient descent, it is common to train the network on the same data repeatedly until convergence. In each iteration, the training data is randomly shuffled and grouped into new batches.\n",
    "\n",
    "Optional parameters may also include:\n",
    "* subdir (defaults to \".\"): The subdirectory (under the object's *basedir* declared above) into which to save the weights and validation data. 'subdir' **is** created if it does not exist\n",
    "* logfile (defaults to None): The file into which to write the training status. If 'None', then output is written to the standard output (e.g. command line). Appends to the file if it already exists. Requires an absolute path to enable integration of the class with other code.\n",
    "* vebose (defaults to True): If this is set to 'True', then detailed outputs are shown (including the time remaining and the number of batches processed in the current epoch). If 'False', then only the validation accuracy at the end of the epoch is shown.\n",
    "* start_epoch (defaults to 0): It may be necessary to continue training from a specific epoch and retain consistency with the epoch naming. This is the epoch at which to continue training.\n",
    "\n",
    "The *DNN* class uses the validation subset of the training data to perform a cross-validation of the state of the network. The calculation and file writing are handled entirely by the *Validator* class (and its *save_results()* method). The file contains a python dictionary with the keys:\n",
    "* precision/recall/counts: Precision/Recall statistics for each image class. This also includes the number of elements identified for each category (correctly or incorrectly) for the purpose of determining a weighted average of PR curves.\n",
    "* top_1_acc: The 'top-1 accuracy', i.e. the number of elements correctly identified with their proper labels\n",
    "* cross_entropy_loss/rmse_loss: The mean values of the loss functions (cross entropy and root mean squared error) for the validation data. Both of these are calculated, regardless of which loss function was actually used for the training, for informational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['recall', 'precision', 'rmse_loss', 'top_1_acc', 'cat_collapse_acc', 'counts', 'cross_entropy_loss']\n",
      "Validation Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"dnn_output/weights/val_0.pkl\", \"r\") as f:\n",
    "    val_dat = pickle.load(f)\n",
    "print \"Keys:\", val_dat.keys()\n",
    "print \"Validation Accuracy:\", val_dat[\"top_1_acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The trained parameters ('weights') are output as as pickled python dictionary. For each node with a set of parameters (i.e. convolutions and all derivatives), two keys exist in the dictionary, 'node_name/w' and 'node_name/b' for the multiplicative weights and the biases, respectively. The values of each dictionary entry is a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1/b : (32,)\n",
      "conv1/w : (3, 3, 1, 32)\n",
      "conv2/b : (32,)\n",
      "conv2/w : (3, 3, 32, 32)\n",
      "conv3/b : (64,)\n",
      "conv3/w : (3, 3, 32, 64)\n",
      "conv4/b : (64,)\n",
      "conv4/w : (3, 3, 64, 64)\n",
      "fc1/b : (2,)\n",
      "fc1/w : (2, 2, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(\"dnn_output/weights/weights_0.pkl\", \"r\") as f:\n",
    "    weights = pickle.load(f)\n",
    "keys = sorted(weights.keys())\n",
    "for key in keys:\n",
    "    print key, \":\", weights[key].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If initializing a DNN with non-random weights, the dictionary **must** have two keys for each layer that requires parameters with the expected shape: (spatial, spatial, input channels, num_kernels/output_channels)\n",
    "\n",
    "## Applying the network\n",
    "The DNN, once trained, can also be applied to images. The output is a vector of (unnormalized) probabilities for each category (the indices of the output vectors correspond to the numerical values given to each category as defined above). There are several \"extra\" dimensions in the numpy array due to tensorflow's internal logic. For consistency, these are included in the output and can be 'squeezed' away for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 1, 1, 2)\n",
      "Output:\n",
      "[[  6.32994604e+00   2.65618057e+01]\n",
      " [  3.14977169e+00   9.42996693e+00]\n",
      " [  8.00156784e+00   4.46643174e-01]\n",
      " [  6.08139038e+00   2.32556953e+01]\n",
      " [  1.31521511e+01   9.78497982e-01]\n",
      " [  2.64972854e+00   1.03727827e+01]\n",
      " [  9.80003834e-01   1.47008562e+00]\n",
      " [  3.31285906e+00   4.99898243e+00]\n",
      " [  1.37556057e+01  -2.84361839e-03]\n",
      " [  4.25528145e+00   1.25044174e+01]]\n",
      "Image Categories: {'Circle': 0, 'Square': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the object with the optimal set of weights\n",
    "trainer = DNNSwift.DNN(\n",
    "    img_dims=img_handler.get_image_dims(), \n",
    "    labels=img_handler.get_image_groups(), \n",
    "    layer_params=dnn_layout, \n",
    "    basedir=\"dnn_output\",\n",
    "    weights=weights)\n",
    "\n",
    "images = img_handler.get_images(\"test\", 0, 9)\n",
    "output = trainer.run_network(input_images=images.imgs)\n",
    "print \"Output shape:\", output.shape\n",
    "print \"Output:\\n\", np.squeeze(output)\n",
    "print \"Image Categories:\", img_handler.get_image_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Comparing the input labels with the output logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Output: [1 1 0 1 0 1 1 1 0 1]\n",
      "Ground Truth:   [1 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print \"Network Output:\", np.argmax(np.squeeze(output), axis=1)\n",
    "print \"Ground Truth:  \", np.argmax(images.labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optional parameters for 'run_network()':\n",
    "* batch_size: Tensorflow can process multiple images simultaneously. However, attempting to load too many images into the network simultaneously leads to a memory bottleneck and drastically slows down operation. Instead, images are loaded into batches (default size: 1000) and batches are then processed in sequence. The optimal batch size may be very system dependent so if you experience unexpected slowdowns when increasing the number of input images, play around with this number.\n",
    "\n",
    "The package also supports running a network on input images of a different size than the images trained on. This can be useful for segmentation if the network is trained on small segments (e.g. foreground/background) of full images. To apply the network to a different image size, the DNN object must be instantiated again with a different *img_dims* argument. For example, let us construct an artificial image from four training images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Image shape: (1, 50, 50, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116990890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nen/2HKUuYksAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104fad050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = img_handler.get_images(\"test\", 0, 3)\n",
    "big_image = np.concatenate(\n",
    "    (np.concatenate((images.imgs[0], images.imgs[1]), axis=0),\n",
    "     np.concatenate((images.imgs[2], images.imgs[3]), axis=0)),\n",
    "    axis=1)\n",
    "big_image = np.expand_dims(big_image, axis=0)\n",
    "print \"Big Image shape:\", big_image.shape\n",
    "\n",
    "plt.imshow(big_image[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then initialize the network and run the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13607b210>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI3CgsP8C4lrMsT6AiRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1088c0490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the same weights as above\n",
    "trainer = DNNSwift.DNN(\n",
    "    img_dims=(50, 50, 1), \n",
    "    labels=img_handler.get_image_groups(), \n",
    "    layer_params=dnn_layout, \n",
    "    basedir=\"dnn_output\",\n",
    "    weights=weights)\n",
    "\n",
    "output = trainer.run_network(input_images=big_image)\n",
    "output_argmax = np.squeeze(np.argmax(output, axis=3))\n",
    "plt.imshow(output_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output is no longer a single probability vector but a map of probabilities (shown here are only the predicted classes). This corresponds to a segmentation map of the input image. Note that this example is a terrible segmentation because the network was not trained to recognize backgrounds, i.e. it expects each pixel to be either a circle or a square.\n",
    "\n",
    "Due to the pooling layers and the padding at the edges of convolutions, the output map is smaller than the input image. I have found that if the number of pooling layers is kept small, simply resizing the output map to the original size gives acceptable results. Future versions of tensorflow will contain unpooling layers that will be integrated into the package.\n",
    "\n",
    "Note that by passing an explicit set of weights to the DNN object, the fully connected layer logic (of reducing the feature map to a single value) is effectively bypassed, making it possible to flexibly use this package for segmentation and classification simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
